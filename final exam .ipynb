{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2054415: expected 9 fields, saw 10\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.840</td>\n",
       "      <td>18.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.630</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.290</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.740</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.680</td>\n",
       "      <td>15.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time Global_active_power Global_reactive_power  Voltage  \\\n",
       "0  16/12/2006  17:24:00               4.216                 0.418  234.840   \n",
       "1  16/12/2006  17:25:00               5.360                 0.436  233.630   \n",
       "2  16/12/2006  17:26:00               5.374                 0.498  233.290   \n",
       "3  16/12/2006  17:27:00               5.388                 0.502  233.740   \n",
       "4  16/12/2006  17:28:00               3.666                 0.528  235.680   \n",
       "\n",
       "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
       "0           18.400          0.000          1.000            17.0  \n",
       "1           23.000          0.000          1.000            16.0  \n",
       "2           23.000          0.000          2.000            17.0  \n",
       "3           23.000          0.000          1.000            17.0  \n",
       "4           15.800          0.000          1.000            17.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset into python\n",
    "df = pd.read_csv('household_power_consumption.txt', delimiter=';',low_memory=False, error_bad_lines=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(2030225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                     1/11/2010\n",
       "Time                      04:13:00\n",
       "Global_active_power          0.274\n",
       "Global_reactive_power        0.000\n",
       "Voltage                    249.050\n",
       "Global_intensity             1.000\n",
       "Sub_metering_1               0.000\n",
       "Sub_metering_2               0.000\n",
       "Sub_metering_3                 1.0\n",
       "Name: 2030226, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2030225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2030225</th>\n",
       "      <td>26/10/2010</td>\n",
       "      <td>249.310</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date     Time Global_active_power Global_reactive_power  \\\n",
       "2030225  26/10/2010  249.310               1.200                 0.000   \n",
       "\n",
       "        Voltage Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
       "2030225   0.000            0.000            NaN            NaN             NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                     26/10/2010\n",
       "Time                        249.310\n",
       "Global_active_power           1.200\n",
       "Global_reactive_power         0.000\n",
       "Voltage                       0.000\n",
       "Global_intensity              0.000\n",
       "Sub_metering_1                  NaN\n",
       "Sub_metering_2                  NaN\n",
       "Sub_metering_3                  NaN\n",
       "Name: 2030225, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2030225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns after removing missing values: (2033256, 2)\n",
      "The time series starts from:  2006-12-16 17:24:00\n",
      "The time series ends on:  2010-12-11 23:59:00\n"
     ]
    }
   ],
   "source": [
    "df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')\n",
    "df = df.dropna(subset=['Global_active_power'])\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "df = df.loc[:, ['date_time', 'Global_active_power']]\n",
    "df.sort_values('date_time', inplace=True, ascending=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Number of rows and columns after removing missing values:', df.shape)\n",
    "print('The time series starts from: ', df['date_time'].min())\n",
    "print('The time series ends on: ', df['date_time'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dates: 2010-12-05 00:00:00 to 2010-12-11 23:59:00\n",
      "Validation dates: 2010-11-23 12:40:00 to 2010-12-04 23:59:00\n",
      "Train dates: 2006-12-16 17:24:00 to 2010-11-17 23:19:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split into training, validation and test datasets.\n",
    "# Since it's timeseries we should do it by date.\n",
    "test_cutoff_date = df['date_time'].max() - timedelta(days=7)\n",
    "val_cutoff_date = test_cutoff_date - timedelta(days=14)\n",
    "\n",
    "df_test = df[df['date_time'] > test_cutoff_date]\n",
    "df_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\n",
    "df_train = df[df['date_time'] <= val_cutoff_date]\n",
    "\n",
    "#check out the datasets\n",
    "print('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max()))\n",
    "print('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max()))\n",
    "print('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 157 files.\n",
      "ts_data/ts_file0.pkl\n",
      "ts_data/ts_file10.pkl\n",
      "ts_data/ts_file20.pkl\n",
      "ts_data/ts_file30.pkl\n",
      "ts_data/ts_file40.pkl\n",
      "ts_data/ts_file50.pkl\n",
      "ts_data/ts_file60.pkl\n",
      "ts_data/ts_file70.pkl\n",
      "ts_data/ts_file80.pkl\n",
      "ts_data/ts_file90.pkl\n",
      "ts_data/ts_file100.pkl\n",
      "ts_data/ts_file110.pkl\n",
      "ts_data/ts_file120.pkl\n",
      "ts_data/ts_file130.pkl\n",
      "ts_data/ts_file140.pkl\n",
      "ts_data/ts_file150.pkl\n"
     ]
    }
   ],
   "source": [
    "# Goal of the model:\n",
    "#  Predict Global_active_power at a specified time in the future.\n",
    "#   Eg. We want to predict how much Global_active_power will be ten minutes from now.\n",
    "#       We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t+10\n",
    "\n",
    "\n",
    "def create_ts_files(dataset, \n",
    "                    start_index, \n",
    "                    end_index, \n",
    "                    history_length, \n",
    "                    step_size, \n",
    "                    target_step, \n",
    "                    num_rows_per_file, \n",
    "                    data_folder):\n",
    "    assert step_size > 0\n",
    "    assert start_index >= 0\n",
    "    \n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    \n",
    "    time_lags = sorted(range(target_step+1, target_step+history_length+1, step_size), reverse=True)\n",
    "    col_names = [f'x_lag{i}' for i in time_lags] + ['y']\n",
    "    start_index = start_index + history_length\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_step\n",
    "    \n",
    "    rng = range(start_index, end_index)\n",
    "    num_rows = len(rng)\n",
    "    num_files = math.ceil(num_rows/num_rows_per_file)\n",
    "    \n",
    "    # for each file.\n",
    "    print(f'Creating {num_files} files.')\n",
    "    for i in range(num_files):\n",
    "        filename = f'{data_folder}/ts_file{i}.pkl'\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'{filename}')\n",
    "            \n",
    "        # get the start and end indices.\n",
    "        ind0 = i*num_rows_per_file\n",
    "        ind1 = min(ind0 + num_rows_per_file, end_index)\n",
    "        data_list = []\n",
    "        \n",
    "        # j in the current timestep. Will need j-n to j-1 for the history. And j + target_step for the target.\n",
    "        for j in range(ind0, ind1):\n",
    "            indices = range(j-1, j-history_length-1, -step_size)\n",
    "            data = dataset[sorted(indices) + [j+target_step]]\n",
    "            \n",
    "            # append data to the list.\n",
    "            data_list.append(data)\n",
    "\n",
    "        df_ts = pd.DataFrame(data=data_list, columns=col_names)\n",
    "        df_ts.to_pickle(filename)\n",
    "            \n",
    "    return len(col_names)-1\n",
    "\n",
    "global_active_power = df_train['Global_active_power'].values\n",
    "\n",
    "# Scaled to work with Neural networks.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "global_active_power_scaled = scaler.fit_transform(global_active_power.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_data')\n",
    "\n",
    "# I found that the easiest way to do time series with tensorflow is by creating pandas files with the lagged time steps (eg. x{t-1}, x{t-2}...) and \n",
    "# the value to predict y = x{t+n}. We tried doing it using TFRecords, but that API is not very intuitive and lacks working examples for time series.\n",
    "# The resulting file using these parameters is over 17GB. If history_length is increased, or  step_size is decreased, it could get much bigger.\n",
    "# Hard to fit into laptop memory, so need to use other means to load the data from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# So we can handle loading the data in chunks from the hard drive instead of having to load everything into memory.\n",
    "# \n",
    "# The reason we want to do this is so we can do custom processing on the data that we are feeding into the LSTM.\n",
    "# LSTM requires a certain shape and it is tricky to get it right.\n",
    "#\n",
    "class TimeSeriesLoader:\n",
    "    def __init__(self, ts_folder, filename_format):\n",
    "        self.ts_folder = ts_folder\n",
    "        \n",
    "        # find the number of files.\n",
    "        i = 0\n",
    "        file_found = True\n",
    "        while file_found:\n",
    "            filename = self.ts_folder + '/' + filename_format.format(i)\n",
    "            file_found = os.path.exists(filename)\n",
    "            if file_found:\n",
    "                i += 1\n",
    "                \n",
    "        self.num_files = i\n",
    "        self.files_indices = np.arange(self.num_files)\n",
    "        self.shuffle_chunks()\n",
    "        \n",
    "    def num_chunks(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def get_chunk(self, idx):\n",
    "        assert (idx >= 0) and (idx < self.num_files)\n",
    "        \n",
    "        ind = self.files_indices[idx]\n",
    "        filename = self.ts_folder + '/' + filename_format.format(ind)\n",
    "        df_ts = pd.read_pickle(filename)\n",
    "        num_records = len(df_ts.index)\n",
    "        \n",
    "        features = df_ts.drop('y', axis=1).values\n",
    "        target = df_ts['y'].values\n",
    "        \n",
    "        # reshape for input into LSTM. Batch major format.\n",
    "        features_batchmajor = np.array(features).reshape(num_records, -1, 1)\n",
    "        return features_batchmajor, target\n",
    "    \n",
    "    # this shuffles the order the chunks will be outputted from get_chunk.\n",
    "    def shuffle_chunks(self):\n",
    "        np.random.shuffle(self.files_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_folder = 'ts_data'\n",
    "filename_format = 'ts_file{}.pkl'\n",
    "tss = TimeSeriesLoader(ts_folder, filename_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Keras model.\n",
    "# Use hyperparameter optimization if you have the time.\n",
    "\n",
    "ts_inputs = tf.keras.Input(shape=(num_timesteps, 1))\n",
    "\n",
    "# units=10 -> The cell and hidden states will be of dimension 10.\n",
    "#             The number of parameters that need to be trained = 4*units*(units+2)\n",
    "x = layers.LSTM(units=10)(ts_inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the training configuration.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1008, 1)]         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "100/100 [==============================] - 33s 334ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0021 - mse: 0.0021\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.0063 - mse: 0.0063\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0073 - mse: 0.0073\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0068 - mse: 0.0068\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 29s 285ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0014 - mse: 0.0014\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 29s 285ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0031 - mse: 0.0031\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 30s 296ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0031 - mse: 0.0031\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 29s 294ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 28s 285ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0064 - mse: 0.0064\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 4.8184e-04 - mse: 4.8184e-04\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0017 - mse: 0.0017\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0082 - mse: 0.0082\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 29s 285ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0019 - mse: 0.0019\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 29s 285ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0032 - mse: 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0019 - mse: 0.0019\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.0029 - mse: 0.0029\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 28s 285ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0027 - mse: 0.0027\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 29s 284ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 29s 289ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 30s 297ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0075 - mse: 0.0075\n",
      "100/100 [==============================] - 29s 285ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0028 - mse: 0.0028\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0022 - mse: 0.0022\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0019 - mse: 0.0019\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.0014 - mse: 0.0014\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 29s 293ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 29s 288ms/step - loss: 0.0046 - mse: 0.0046\n",
      "CPU times: user 2h 20min 21s, sys: 22min 7s, total: 2h 42min 28s\n",
      "Wall time: 1h 20min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train in batch sizes of 128.\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1\n",
    "NUM_CHUNKS = tss.num_chunks()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    for i in range(NUM_CHUNKS):\n",
    "        X, y = tss.get_chunk(i)\n",
    "        \n",
    "        # model.fit does train the model incrementally. ie. Can call multiple times in batches.\n",
    "        # https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(x=X, y=y, batch_size=BATCH_SIZE)\n",
    "        \n",
    "    # shuffle the chunks so they're not in the same order next time around.\n",
    "    tss.shuffle_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the validation set.\n",
    "#\n",
    "# Create the validation CSV like we did before with the training.\n",
    "global_active_power_val = df_val['Global_active_power'].values\n",
    "global_active_power_val_scaled = scaler.transform(global_active_power_val.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 0 files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_val_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_val_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ts_val_data/ts_file0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b91ad48e799c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If we assume that the validation dataset can fit into memory we can do this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_val_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ts_val_data/ts_file0.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_val_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     ) as handles:\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ts_val_data/ts_file0.pkl'"
     ]
    }
   ],
   "source": [
    "# If we assume that the validation dataset can fit into memory we can do this.\n",
    "df_val_ts = pd.read_pickle('ts_val_data/ts_file0.pkl')\n",
    "\n",
    "\n",
    "features = df_val_ts.drop('y', axis=1).values\n",
    "features_arr = np.array(features)\n",
    "\n",
    "# reshape for input into LSTM. Batch major format.\n",
    "num_records = len(df_val_ts.index)\n",
    "features_batchmajor = features_arr.reshape(num_records, -1, 1)\n",
    "\n",
    "\n",
    "y_pred = model.predict(features_batchmajor).reshape(-1, )\n",
    "y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "y_act = df_val_ts['y'].values\n",
    "y_act = scaler.inverse_transform(y_act.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "print('validation mean squared error: {}'.format(mean_squared_error(y_act, y_pred)))\n",
    "\n",
    "#baseline\n",
    "y_pred_baseline = df_val_ts['x_lag11'].values\n",
    "y_pred_baseline = scaler.inverse_transform(y_pred_baseline.reshape(-1, 1)).reshape(-1 ,)\n",
    "print('validation baseline mean squared error: {}'.format(mean_squared_error(y_act, y_pred_baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
